---
layout: default
---

# TACO Truck

TACO Truck is an umbrella term for Stanford Libraries' prototype and implementation of COCINA. This was formerly called SDR3 (standing for the third iteration of the Stanford Digital Repository). We've moved away from the term SDR3, as there won't be a clear cut-over between systems (i.e. we won't just turn off SDR2, turn on SDR3, and be done). In fact, TACO Truck emerged from needing something that could be replaced piecemeal, and the implementation is part maintenance of existing components we know we'll be keeping, part prototype of the components we know we need to replace.

One of the biggest disconnects we had when working on earlier iterations of a possible new architecture (such as Hyrax, Hyku, or Fedora 4 early work) was an understanding what our current system does and what new systems would replace or extend. This disconnect was largely due to the complexity of our current system, how it grew over a decade with many different people involved, and knowledge/experience silos within our teams (i.e. Hydra-mainly developers and SDR2-mainly developers). To address this, the following areas of work were involved:
* We performed a [SDR domain](#taco-truck-domain) & [baseline functional requirements analysis](#baseline-functional-requirements) with a cross-teams design group.
* This was paired with a [SDR2 Current State exercise](#sdr-current-state), to understand how our current infrastructure & codebases map to our high level designs.
* We also performed a [DLSS-wide retrospective on SDR2](#sdr2-department-wide-retrospective), to understand what pain points & technology needs we should prioritize.
* Based on the above, we then assessed existing technologies - Hyrax, Fedora 4, Fedora API, Valkyrie - for our needs.
* We set out goals for what became COCINA, or our architectural approach for the given domain and requirements.
* We prototyped the core components of COCINA, namely TACO (our repository back-end service).
* And we created a TACO Truck roadmap and migration plan.

All of these above points are represented below, with more information on that work.

### TACO Truck Domain

We first performed a conceptual exercise to understand what the domain of SDR is and what we were trying to re-design (and why). We generated this definition of our domain:

> Digital repository system, based on SUL’s needs, that ingests, processes, enhances, manages, stores, and provisions non-preservation online/digital objects, namely, some grouping of binaries with contextual information (both provided by users and generated by the system). Basically, the infrastructure that makes information available for administration and for all of our digital library preservation and public-facing delivery, discovery and access services. The edges of SDR3 are flexible, but interfaces (used broadly) of the domain’s data is in scope.

It is important to note here the general topology of our current system:

![](assets/img/sdr2_three_spheres.png)

This loosely guided our architecture and implementations for SDR2. There are meant to be (i.e. they are designed but not necessarily implemented) hard interface gaps between Management - our administration, ingest, and repository processing applications; preservation - our homegrown, long-term digital preservation framework that takes MOAB resources from our management layer and preserves them; and access - our files and metadata for discovery and delivery through our online catalog, user collection applications like our instance of Spotlight, a persistent URL resolver for each object and collection, and APIs for retrieval of publicly accessible metadata and assets.

Given this separation, we scoped the domain of TACO Truck to primarily that Management layer, including the interfaces to Preservation and Access. One reason for including the interfaces is that our current implementation of this architecture does not respect these spheres, meaning the system can be extremely hard to debug or extend and test fully. Additionally, changes in Management expectations will then effect downstream systems in, we hope, positive ways - clearer data models that are shared system-wide; cleaner, less coupled data hand-offs, etc. We go into this more below.

For us, however, it means that TACO Truck does include:
* Storage & management system that interfaces with (or provisions to) other systems (preservation, discovery).
* Data administration and ingest / feeder components to our system.

And TACO Truck here does not include:
* Preservation Functions;
* Access & Discovery Systems.

### Baseline Functional Requirements

Before diving further into what we have, it is worth showing our work understanding SDR2 in terms of what baseline requirements are currently served. when we say baseline requirements, we mean what we know SDR2 currently does within our scoped domain definition. This is not every requirement for all users of SDR2, and it definitely is not a comprehensive list of requests and improvements coming from use cases or user stories. Instead, this was an exercise for the cross-department design team to build a common understanding of what SDR2 currently does so that TACO Truck would remain at functional parity. As we continue work on TACO Truck, we are currently lining up meetings with key stakeholder groups to enrich and expand this set of requirements.

You can see a backlog of the requirements as Github tickets here: https://waffle.io/sul-dlss/taco-truck This Waffle board is a work in progress as we clean-up and migrate over the requirements and discussion notes that we generated originally here: https://docs.google.com/document/d/186XkuL_gpd6mhCss9-PC8LofqSNuD7E-Q24wBZIVb4Y/edit.

Additionally, this work generated a [Glossary](Glossary.html) of the terms we use in this project for consistency sake; and documentation on aspects like [what does "delete" mean within SDR (past, present, or future)](https://docs.google.com/document/d/1zBr93MRg8wJUE7QMpOkWB8R6XWWfEkiUUz_o2T6FIC4/edit).

### SDR Current State

Current State was a small working group to explore, explain, document, and improve shared understanding of the current state of the SDR2. The goal was to start with all the places where a resource (metadata and possibly some sort of digital asset) would enter SDR then follow it through all the machines, codebases, scripts, databases, etc. until it ended up in Preservation and in our discovery layers. This was taking what we learned above and then see how it is currently implemented, and was sorely needed work given the complexity and size of our digital repository system implementation.

Before jumping into what we found there, it is worth calling out what SDR currently involves. Here are some facts:

* We have around 1,618,552 resources (collections, items ["work" not file level abstraction], and related administrative objects) currently in our repository.
* We keep all SDR resource metadata in Fedora 3. Digital assets are kept in an NFS mount called "workspace" until they are confirmed to be in Preservation and in Access, then they are cleared out from the repository. This is purely a cost-saving function, as Preservation holds ~455 TB of digital assets currently, Access holds ~426 TB of digital assets, and we have hundreds of TBs of digital assets on our ingest backlog.
* Not all resources are preserved, and not all resources are publicly discoverable / accessible. We currently have ~248,232 items that are 'dark' - i.e. preserved but not served up for public access. Many derivative files for access usage only are not preserved.
* There are at least 5 primary ingest points used by a wide variety of users (see more about each below).
* SDR spans many different content types:
    * images (archival assets)
    * PDFs (ETDs, Google Books, self-deposit uploads, ...)
    * datasets
    * maps & atlases
    * web archiving materials
    * streaming media
    * etc.

Whereas some places may have multiple systems to deal with each bucket of content or users separately, SDR is the single, core system of a lot of divergent needs, materials, workflows, and user groups. This makes understanding our current system en toto vital but at times difficult. To break this up for the Current State review group, we started by breaking apart the key codebases involved and grouping them roughly according to function:

![Current State Overview Diagram](https://docs.google.com/drawings/d/e/2PACX-1vS0eWlm7_ETunGzpkX0-_2VnZL1N7NpqfmFivPaUbWWfwfi5TXZeKDEMGxIEDT-XCTq9ylutKlb34Ks/pub?w=960&h=720)

**Ingest**
The most important ingest components are:
* [Hydrus](https://github.com/sul-dlss/hydrus) is our primary self-deposit app, used for institutional repository, cultural heritage, free online resource cataloging, and other ingest. It is 6 years old, and started with hydra-head 4.0.0, so is an early Hydra application.
* [ETDs](https://github.com/sul-dlss/hydra_etd) (fyi, private repository) is, loosely, a self-deposit application primarily for managing approval and submission of ETDs to the registrar and SDR concurrently. It is nearly 10 years old, and is considered a "proto-hydra" application in that the structure of this codebase influenced early Hydra work.
* [Pre-Assembly](https://github.com/sul-dlss/pre-assembly) is a set of Ruby scripts used heavily by the DLSS Accessioning team to do bulk ingest and curated deposit. It is 6 years old and interacts heavily with our "workflows and robots framework", a combination of a resource state machine and declarative-workflow-driven asynchronous processing framework. The majority of our materials - from our various digitization labs or other sources - enters SDR through Pre-Assembly.
* [WAS-Registrar](https://github.com/sul-dlss/was-registrar) is a Rails application for ingesting archived websites. It is relatively new, being 4 years old, and is one step in mostly manual process for loading web archiving assets into SDR.
* [Argo](https://github.com/sul-dlss/argo) is a special mention here, as it is primarily an administration dashboard for resources in SDR. Through some additional codebases, it allows bulk editing of metadata through CSV upload, along with review of the status of resources going through SDR - including a huge index behind a Blacklight interface for administrative search and discovery, and a redis-driven dashboard showing the status of resources going through our state machine and asynchronous processing. It is a bit over 7 years old and sits at the heart of a lot of the administrative users needs for this system.

Many of these ingest points connect to Fedora 3 through ActiveFedora, Dor-Services(-App) and connect to Dor-Workflow-Service for processing (discussed below).

There were a number of pseudo-data-flow type diagrams made to understand what these codebases were doing when someone kicked off an ingest, and this work provided the starting interfaces and data shapes for the next review group. Here are two examples:

*Hydrus Current State "Dataflows"*
![Hydrus diagram](https://docs.google.com/drawings/d/e/2PACX-1vSLJXuphIH4_La3eYiiPY3zIuc46cJVPBQ2w_OKA2mmq0vqR0CIt6kZmGpQQhl_SCSL13kdNHobmuZI/pub?w=2527&h=1690)

*Pre-assembly Current State "Dataflows"*
![Pre-assembly diagram](https://docs.google.com/drawings/d/e/2PACX-1vR8X5NbWjdxiw7K5OKEGlj0t4TrK5_IxcU-2LzDMf3Ph5wpS2FFQf68rBf5xqHezLqPxjuo4JcQNoR3/pub?w=2091&h=1459)

*WAS Registrar (web archiving) "Dataflow"*
![Web Archiving System diagram](https://docs.google.com/drawings/d/e/2PACX-1vQMPWuqrO5sIFNnMov6fLcgVEXx8-TzR9KuYCVv3GRjwkRwyFfyP1NF5Dpm5bIYSIwl9fYTs9WT31pl/pub?w=960&h=720)

**DOR Services**




**Robots & Workflows**


**Argo+**


**PURL+**


**Preservation**


**Stacks / Shelves**


**Indexing, Access & Discovery**

### SDR2 Department-wide Retrospective

WIP.

### TACO Truck Prototypes

WIP.

In Winter-Spring 2018, we have a time-boxed work cycle to create a prototype of TACO.

* https://github.com/sul-dlss-labs/taco
* https://github.com/sul-dlss-labs/identifier-service
* https://github.com/sul-dlss-labs/sdr3-models

### Community Overlaps

WIP.

**Fedora Review**

* **Incompleteness:** F4? F5? API is incomplete; unclear about Fixity & Notifications; community guidance uncertainty.
* **Complexity & Comprehensibility:** Fedora API is very complex; encompasses LDP, WebACL, Memento.
* **Graph Store Limitations:** Need record management, not just statement management. See Europeana Case.
* **Performance & Extensibility:** Number of calls required for building a resource is higher with LDP & Fedora API.
  * LDP: Goes beyond using RDF to publishing Linked Data. We are not publishing at the TACO level.
  * WebACL & Complex Permission Handling: SDR3 has separate Permission Service for multiple systems to use.
    * Permissions as resource properties (metadata) vs WebACL requests - consider for use cases like Argo.
* **Data & Resource Handling:** JSON-LD support; Do not want quad store or split graphs, but need record management.
  * Resource Ordering is already hard in RDF; more so with LDP & Fedora API.
  * TACO semantically validates our complex data shapes; Fedora API does not do this flexibly.
  * No query, filtering, constraint support. See SDR3 use cases requiring this (ex: de-duplicate sourceIDs).
* **System Expectations:** Monoliths? Need multiple systems with differing contexts (Admin, Process) to work w/TACO.

See our blog posts on our Fedora analysis here: TBD


**Hyrax Review**


**Valkyrie Review**


**PCDM**


**IIIF**


### TACO Truck Roadmap & Migration Plans
